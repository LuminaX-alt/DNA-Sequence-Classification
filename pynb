import numpy as np

def generate_dna_sequence(length):
    return ''.join(np.random.choice(['A', 'C', 'G', 'T'], length))

def generate_dataset(num_samples, seq_length=100):
    sequences = []
    labels = []
    for _ in range(num_samples):
        seq = generate_dna_sequence(seq_length)
        # Let's say sequences with more 'G's belong to class 1 (disease), else 0
        label = 1 if seq.count('G') > seq_length / 4 else 0
        sequences.append(seq)
        labels.append(label)
    return sequences, np.array(labels)

# Example usage
sequences, labels = generate_dataset(1000)
print(sequences[:2], labels[:2])
def one_hot_encode(seq):
    mapping = {'A': [1,0,0,0], 'C': [0,1,0,0], 'G': [0,0,1,0], 'T': [0,0,0,1]}
    return np.array([mapping.get(base, [0,0,0,0]) for base in seq])

# Encode all sequences
X = np.array([one_hot_encode(seq) for seq in sequences])
print(X.shape)  # Should be (num_samples, seq_length, 4)
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, Flatten

model = Sequential([
    Conv1D(64, 5, activation='relu', input_shape=(100,4)),
    MaxPooling1D(2),
    LSTM(64, return_sequences=False),
    Dense(32, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.summary()
history = model.fit(X, labels, epochs=10, batch_size=32, validation_split=0.2)
test_seq = generate_dna_sequence(100)
test_encoded = np.expand_dims(one_hot_encode(test_seq), axis=0)
pred = model.predict(test_encoded)[0][0]
print(f"Prediction score: {pred:.4f} for sequence:\n{test_seq}")
import sys
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import sklearn

plt.rcParams['figure.figsize'] = (8, 8)
print('Python: {}'.format(sys.version))
print('Numpy: {}'.format(np.__version__))
print('Pandas: {}'.format(pd.__version__)
      # UCI molecular biology (promoter gene sequences dataset)
url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/molecular-biology/promoter-gene-sequences/promoters.data'
names = ['Class', 'id', 'Sequence']

data = pd.read_csv(url, names=names)
data.head()
# build dataset using a custom pandas dataframe
# each column in a dataframe is called a series
classes = data.loc[:, 'Class']
classes.value_counts()
# Generate list of DNA sequences
sequences = data.loc[:, 'Sequence'].tolist()
dataset = {}

# Loop throught the sequences and split into individual nucleotides
for i, seq in enumerate(sequences):
    # split into nucleotides, remove tab characters
    nucleotides = list(seq)
    nucleotides = [x for x in nucleotides if x != '\t']
    
    # Append class assignment
    nucleotides.append(classes[i])
    
    # add to dataset
    dataset[i] = nucleotides
    
print(dataset[0])
# Turn dataset into pandas dataframe
df = pd.DataFrame(dataset).T
df
# rename the last column to class
df.rename(columns={57: 'Class'}, inplace=True)
df
df.describe()
# Record value counts for each sequence
series = []

for name in df.columns:
    series.append(df[name].value_counts())
    
info = pd.DataFrame(series)
details = info.T
details
# Switch to numerical data using pd.get_dummies()
numerical_df = pd.get_dummies(df)
numerical_df.head()
# Remove one of the class columns and rename to simply 'Class'
df = numerical_df.drop(columns=['Class_-'])

df.rename(columns={'Class_+':'Class'}, inplace=True)
df
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score
from sklearn.model_selection import KFold, train_test_split, cross_val_score
# Create X and y dataset for training
X = df.drop(['Class'], axis=1).to_numpy()
y = df['Class'].to_numpy()

# Split the data into training and test dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle=True)
# Define scoring method
scoring = 'accuracy'

# Define models to train
names = ['K Nearest Neighbors', 'Gaussian Process', 'Decision Tree', 'Random Forest', 
         'Neural Network', 'AdaBoost', 'Naive Bayes', 'SVM Linear', 'SVM RBF', 'SVM Sigmoid']

classifiers = [
    KNeighborsClassifier(n_neighbors=3),
    GaussianProcessClassifier(1.0 * RBF(1.0)),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    MLPClassifier(alpha=1, max_iter=500),
    AdaBoostClassifier(),
    GaussianNB(),
    SVC(kernel='linear'),
    SVC(kernel='rbf'),
    SVC(kernel='sigmoid')
]

models = zip(names, classifiers)

# Evaluate each model in turn
results = []
names = []

for name, model in models:
    kfold = KFold(n_splits=10, shuffle=True)
    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)
    results.append(cv_results)
    names.append(name)
    msg = '{0}:  {1}  ({2})'.format(name, cv_results.mean(), cv_results.std())
  models = zip(names, classifiers)
# Test the algorithm on the validation dataset
for name, model in models:
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    print(name)
    print(accuracy_score(y_test, predictions))
    print(classification_report(y_test, predictions))
  
